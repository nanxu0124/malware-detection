import sys
import os
current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)

import re
import torch
import pickle

import random
import numpy as np

import configparser
import pandas as pd
import torch.nn as nn

from data.vocab import vocab
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

class ALiDataset(Dataset):
    def __init__(self, train=True, sequence_max_len=512):
        self.sequence_max_len = sequence_max_len
        self.df = pd.read_csv('./data/ali_data/data/data_cla2.csv')
        train_df, test_df = train_test_split(self.df, test_size=0.1, random_state=42)

        if train:
            self.df = train_df
        else:
            self.df = test_df

    def __getitem__(self, idx):
        file_id = self.df.iloc[idx]['file_id']
        api = tokenlize(self.df.iloc[idx]['api'])
        remove_dup = tokenlize(self.df.iloc[idx]['remove_dup'])
        label = self.df.iloc[idx]['label']
        return file_id, api, remove_dup, label
    
    def __len__(self):
        return len(self.df)


# 去除每行 "api" 列中连续重复的单词
def remove_consecutive_duplicate_words(text):
    words = text.split()  # 使用空格拆分文本为单词列表
    unique_words = [words[0]]  # 初始化列表，将第一个单词添加到结果中
    for word in words[1:]:
        if word != unique_words[-1]:  # 如果当前单词不等于结果列表中的最后一个单词
            unique_words.append(word)  # 则将当前单词添加到结果列表中
    return ' '.join(unique_words)  # 将去除连续重复单词后的单词重新组合成字符串


def tokenlize(sentence):
    """
    进行文本分词
    :param sentence: str
    :return: [str,str,str]
    """

    fileters = ['!', '"', '#', '$', '%', '&', '\(', '\)', '\*', '\+', ',', '-', '\.', '/', ':', ';', '<', '=', '>',
                '\?', '@', '\[', '\\', '\]', '^', '_', '`', '\{', '\|', '\}', '~', '\t', '\n', '\x97', '\x96', '”',
                '“', ]
    sentence = sentence.lower()  # 把大写转化为小写
    sentence = re.sub("<br />", " ", sentence)
    sentence = re.sub("|".join(fileters), " ", sentence)
    result = [i for i in sentence.split(" ") if len(i) > 0]

    return result


def collate_fn(batch):
    """
    对batch数据进行处理
    :param batch: [一个getitem的结果,getitem的结果,getitem的结果]
    :return: 元组
    """
    reviews, labels = zip(*batch)

    return reviews, labels

def MLData():
    df = pd.read_csv('./data/ali_data/data/data_cla2.csv')
    train_df, test_df = train_test_split(df, test_size=0.3, random_state=32)

    train_data = train_df['remove_dup']
    train_labels = train_df['label']
    test_data = test_df['remove_dup']
    test_labels = test_df['label']
    return train_data, train_labels, test_data, test_labels


# vocab_pkl 保存词表
vocab_pkl = pickle.load(open('./data/ali_data/data/ali_vocab.pkl', 'rb'))

# 创建一个配置解析器对象
config = configparser.ConfigParser()
# 读取INI文件
config.read('./config/config.ini')

# collate_fn_vocab 将api转成词表中对应的数字
def collate_fn_vocab(batch):

    file_id, api, remove_dup, labels = zip(*batch)

    api_lists = []
    api_counts = []
    for i in api:
        api_lists.append(vocab_pkl.transform(i, max_len=int(config['data']['sequence_max_len'])))

    for api_list in api_lists:
        api_count = [0] * 300
        for idx in api_list:
            api_count[idx] += 1
        api_count = api_count[20:100]
        api_counts.append(api_count)
        
    api = torch.LongTensor([vocab_pkl.transform(i, max_len=int(config['data']['sequence_max_len'])) for i in api])
    api_counts = torch.LongTensor(api_counts)
    remove_dup = torch.LongTensor([vocab_pkl.transform(i, max_len=int(config['data']['sequence_max_len'])) for i in remove_dup])
    labels = torch.LongTensor(labels)
    return file_id, api, api_counts, remove_dup, labels


# if __name__ == "__main__":
#     print(1)
#     # 测试dataloader

#     ali_dataset = ALiDataset(train=False)
#     ali_dataloader = DataLoader(ali_dataset, batch_size=500, shuffle=False, collate_fn=collate_fn_vocab)

#     for file_id, api, api_counts, remove_dup, labels in ali_dataloader:

#         print(file_id)
#         # break


# 测试vocab
# seed = 42
# torch.manual_seed(seed)
# torch.cuda.manual_seed(seed)
# random.seed(seed)
# np.random.seed(seed)

# test_dataset = ALiDataset(train=False)
# test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_vocab)

# embedding = nn.Embedding(num_embeddings=len(vocab_pkl), embedding_dim=int(config['data']['embedding_dim']), padding_idx=vocab_pkl.PAD)

# for api, label in test_dataloader:
#     print(api)
#     api = embedding(api)
#     print(api)
#     print(api.shape)

#     break