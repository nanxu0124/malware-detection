import sys
import os
current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)

import re
import torch
import pickle

import random
import numpy as np

import configparser
import pandas as pd
import torch.nn as nn

from data.vocab import vocab
from torch.utils.data import Dataset, DataLoader

class ALiDataset(Dataset):
    def __init__(self, train=True, sequence_max_len=512):
        self.sequence_max_len = sequence_max_len
        file_path = './data/ali_data/data/train.csv' if train else './data/ali_data/data/test.csv'
        self.df = pd.read_csv(file_path)

    def __getitem__(self, idx):
        remove_dup = tokenlize(self.df.iloc[idx]['remove_dup'])
        label = self.df.iloc[idx]['label']
        return remove_dup, label
    
    def __len__(self):
        return len(self.df)


# 去除每行 "api" 列中连续重复的单词
def remove_consecutive_duplicate_words(text):
    words = text.split()  # 使用空格拆分文本为单词列表
    unique_words = [words[0]]  # 初始化列表，将第一个单词添加到结果中
    for word in words[1:]:
        if word != unique_words[-1]:  # 如果当前单词不等于结果列表中的最后一个单词
            unique_words.append(word)  # 则将当前单词添加到结果列表中
    return ' '.join(unique_words)  # 将去除连续重复单词后的单词重新组合成字符串


def tokenlize(sentence):
    """
    进行文本分词
    :param sentence: str
    :return: [str,str,str]
    """

    fileters = ['!', '"', '#', '$', '%', '&', '\(', '\)', '\*', '\+', ',', '-', '\.', '/', ':', ';', '<', '=', '>',
                '\?', '@', '\[', '\\', '\]', '^', '_', '`', '\{', '\|', '\}', '~', '\t', '\n', '\x97', '\x96', '”',
                '“', ]
    sentence = sentence.lower()  # 把大写转化为小写
    sentence = re.sub("<br />", " ", sentence)
    sentence = re.sub("|".join(fileters), " ", sentence)
    result = [i for i in sentence.split(" ") if len(i) > 0]

    return result


def collate_fn(batch):
    """
    对batch数据进行处理
    :param batch: [一个getitem的结果,getitem的结果,getitem的结果]
    :return: 元组
    """
    reviews, labels = zip(*batch)

    return reviews, labels


# vocab_pkl 保存词表
vocab_pkl = pickle.load(open('./data/ali_data/data/ali_vocab.pkl', 'rb'))

# 创建一个配置解析器对象
config = configparser.ConfigParser()
# 读取INI文件
config.read('./config/config.ini')

# collate_fn_vocab 将api转成词表中对应的数字
def collate_fn_vocab(batch):

    api, labels = zip(*batch)

    api = torch.LongTensor([vocab_pkl.transform(i, max_len=int(config['data']['sequence_max_len'])) for i in api])
    labels = torch.LongTensor(labels)
    return api, labels


# if __name__ == "__main__":
#     print(1)
#     # 测试dataloader

#     ali_dataset = ALiDataset(train=False)
#     ali_dataloader = DataLoader(ali_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)

#     for api, label in ali_dataloader:

#         print(api)
#         break


# 测试vocab
# seed = 42
# torch.manual_seed(seed)
# torch.cuda.manual_seed(seed)
# random.seed(seed)
# np.random.seed(seed)

# test_dataset = ALiDataset(train=False)
# test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_vocab)

# embedding = nn.Embedding(num_embeddings=len(vocab_pkl), embedding_dim=int(config['data']['embedding_dim']), padding_idx=vocab_pkl.PAD)

# for api, label in test_dataloader:
#     print(api)
#     api = embedding(api)
#     print(api)
#     print(api.shape)

#     break