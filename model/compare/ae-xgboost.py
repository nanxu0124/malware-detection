import sys
import os

current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)


import random
import numpy as np
import torch
import torch.nn as nn
import configparser
import torch.nn.functional as F
from utils.utils import device, model_save, model_save_v2
from torch.optim import Adam
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import recall_score, f1_score, accuracy_score, precision_score
from data.ali_data.ali_dataset import ALiDataset
from utils.utils import device, create_folder, save_loss, model_info_save, plt_curve, setup_seed
from xgboost import XGBClassifier

from data.ali_data.ali_dataset import vocab_pkl
config = configparser.ConfigParser()
config.read(os.path.join(project_dir, os.path.join("config", "config.ini")))

vocab_size = len(vocab_pkl)
dim = int(config['data']['embedding_dim'])
max_len = int(config['data']['sequence_max_len'])

class AE(nn.Module):
    def __init__(self):
        super(AE, self).__init__()

        self.embedding = nn.Embedding(vocab_size, dim, padding_idx=vocab_pkl.PAD)
        self.encoder_conv = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),

            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1), 
            nn.BatchNorm2d(64),
            nn.ReLU(),

            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
        )

        self.decoder_conv = nn.Sequential(
            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),

            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),

            nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.embedding(x)
        x = x.view(x.size(0), 1, max_len, dim)
        embed_x = x

        latent = self.encoder_conv(x)
        recon_x = self.decoder_conv(latent)

        return embed_x, recon_x, latent



def TrainAEXGBoost(train_loader, folder_path):
    print("AEXGBoost Train begin")

    model = AE().to(device())
    AEXGBoostLoss = []
    epochs = 20
    lr = 0.005
    optimizer = Adam(model.parameters(), lr)

    for epoch in range(epochs):
        running_loss = 0.0
        model.train()
        
        for file_id, api, api_counts, remove_dup, labels, mixture_entropy in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', ncols=100):

            inputs = remove_dup.to(device())
            labels = labels.to(device())
            mixture_entropy = mixture_entropy.to(device())
            mixture_entropy = mixture_entropy.view([128,1])
            inputs[:, -1] = mixture_entropy.view(-1)
            optimizer.zero_grad()
            embed_x, recon_x, latent = model(inputs)
            loss = F.mse_loss(embed_x, recon_x)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_loss = running_loss / len(train_loader)
        AEXGBoostLoss.append(avg_loss)
        print(f'Epoch [{epoch + 1}/{epochs}] - Loss: {avg_loss:.4f}')

    model_save(folder_path, (model,))
    save_loss(folder_path, AEXGBoostLoss, "AEXGBoostLoss")
    print('AEXGBoost Train finished')
    return model


def extract_ae_features(ae, train_loader, folder_path):

    features = []
    labels = []

    # ae = AE().to(device())
    # ae.load_state_dict(torch.load(os.path.join(current_dir, 
    #                                 os.path.join("model_file", 
    #                                 os.path.join("2023-12-29_16-13-01", "AE_model_state_dict.pkl")))))
    
    for file_id, api, api_counts, remove_dup, label, mixture_entropy in train_loader:
        inputs = remove_dup.to(device())
        embed_x, recon_x, latent = ae(inputs)
        features.append(latent.view(latent.size(0), -1).cpu().detach().numpy())
        labels.append(label.cpu().detach().numpy())

    return np.concatenate(features, axis=0), np.concatenate(labels, axis=0)


def TrainXGBoost(ae, train_loader,test_loader, folder_path):
    train_ae_features, train_labels = extract_ae_features(ae, train_loader, folder_path)
    test_ae_features, true_labels = extract_ae_features(ae, test_loader, folder_path)

    xgb_model = XGBClassifier()
    xgb_model.fit(train_ae_features, train_labels)

    predicted_labels = xgb_model.predict(test_ae_features)
    
    recall = recall_score(true_labels, predicted_labels)
    f1 = f1_score(true_labels, predicted_labels)
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision = precision_score(true_labels, predicted_labels)
    print(f'Test Accuracy: {accuracy:.4f}')
    print(f'Test Recall: {recall:.4f}')
    print(f'Test F1-Score: {f1:.4f}')
    print(f'Test precision: {precision:.4f}')

    model_info_save(folder_path, recall, f1, accuracy, precision)




def SetSeed():
    # 设置随机种子
    setup_seed(42)
    torch_state_seed_value = torch.initial_seed()
    print("Torch Random Seed:", torch_state_seed_value)

    rng_state = np.random.get_state()
    rng_state_seed_value = rng_state[1][0]
    print("NumPy Random Seed:", rng_state_seed_value)

    random_state = random.getstate()
    random_state_seed_value = random_state[1][0]
    print("Python Random Seed:", random_state_seed_value)

def collate_fn_vocab(batch):

    file_id, api, remove_dup, labels = zip(*batch)

    api_lists = []
    api_counts = []
    mixture_entropys = []
    for i in api:
        api_lists.append(vocab_pkl.transform(i, max_len=int(config['data']['sequence_max_len'])))

    for api_list in api_lists:
        api_count = [0] * 300
        for idx in api_list:
            api_count[idx] += 1
        api_count_sum = sum(api_count)
        probabilities = [count / api_count_sum for count in api_count]
        epsilon = 1e-10
        probabilities_tensor = torch.tensor(probabilities) + epsilon
        mixture_entropy = -torch.sum(probabilities_tensor * torch.log2(probabilities_tensor))
        mixture_entropy = mixture_entropy.tolist()

        api_counts.append(api_count)
        mixture_entropys.append(mixture_entropy)
    
    api = torch.LongTensor([vocab_pkl.transform(i, max_len=int(config['data']['sequence_max_len'])) for i in api])
    api_counts = torch.LongTensor(api_counts)
    remove_dup = torch.LongTensor([vocab_pkl.transform(i, max_len=int(config['data']['sequence_max_len'])) for i in remove_dup])
    labels = torch.LongTensor(labels)
    mixture_entropys = torch.LongTensor(mixture_entropys)
    return file_id, api, api_counts, remove_dup, labels, mixture_entropys

def BuildDataloader():
    # 构建 train_dataloader
    train_dataset = ALiDataset(train=True)
    train_dataloader = DataLoader(train_dataset, batch_size=128, 
                                  shuffle=True, collate_fn=collate_fn_vocab, drop_last=True)

    # 构建 test_dataloader
    test_dataset = ALiDataset(train=False)
    test_dataloader = DataLoader(test_dataset, batch_size=128, 
                                 shuffle=True, collate_fn=collate_fn_vocab, drop_last=False)
    return train_dataloader, test_dataloader


if __name__ == '__main__':

    SetSeed()
    
    train_dataloader, test_dataloader = BuildDataloader()

    # 创建保存模型文件夹
    folder_path = create_folder(os.path.join(current_dir, "model_file"))
    # folder_path = os.path.join(current_dir, 
    #                                os.path.join("model_file", "2023-12-29_16-13-01"))

    # 训练
    ae = TrainAEXGBoost(train_dataloader, folder_path)
    TrainXGBoost(ae, train_dataloader,test_dataloader, folder_path)
