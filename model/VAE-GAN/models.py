import sys
import os

current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)

import torch
import torch.nn as nn
import torch.nn.functional as F
import configparser

from data.ali_data import ali_dataset

# 创建一个配置解析器对象
config = configparser.ConfigParser()
# 读取INI文件
config.read('./config/config.ini')


vocab_size = len(ali_dataset.vocab_pkl)
dim = int(config['data']['embedding_dim'])
n_class = 8
max_len = int(config['data']['sequence_max_len'])
batch = int(config['VAE-GAN']['batch_size'])

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()

        self.n_channel = 1
        self.dim_h = int(config['VAE-GAN']['dim_h'])
        self.n_z = int(config['VAE-GAN']['latent_size'])

        # 将实现训练好的词向量导入
        # input:([100, 512])
        self.embeding = nn.Embedding(vocab_size, dim, padding_idx=ali_dataset.vocab_pkl.PAD)    # ([100, 512, 512])

        self.main = nn.Sequential(
            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),     # ([100, 128, 256, 256])
            nn.ReLU(True),
            nn.MaxPool2d(2),    # ([100, 128, 128, 128])

            nn.Dropout(0.5),

            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),     # ([100, 256, 64, 64])
            nn.BatchNorm2d(self.dim_h * 2),
            nn.ReLU(True),
            nn.MaxPool2d(2),    # ([100, 256, 32, 32])

            nn.Dropout(0.5),

            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False), # ([100, 512, 16, 16])
            nn.BatchNorm2d(self.dim_h * 4),
            nn.ReLU(True),
            nn.MaxPool2d(2),    # ([100, 512, 8, 8])

            nn.Dropout(0.5),

            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False), # ([100, 1024, 4, 4])
            nn.BatchNorm2d(self.dim_h * 8),
            nn.ReLU(True),
            nn.MaxPool2d(2),    # ([100, 1024, 2, 2])

            nn.Dropout(0.5),

            nn.Conv2d(self.dim_h * 8, self.dim_h * 16, 4, 2, 1, bias=False),# ([100, 2048, 1, 1])
            nn.BatchNorm2d(self.dim_h * 16),
            nn.ReLU(True),

            nn.Dropout(0.5),
        )
        self.fc = nn.Linear(self.dim_h * (2 ** 4), self.n_z)

    def forward(self, x):
        x = self.embeding(x)
        x = x.view(x.size(0), 1, max_len, dim)  # ([100, 1, 512, 512])
        y = x
        x = self.main(x)
        x = x.squeeze()     # ([100, 2048])
        # x = self.fc(x)      # ([100, 8])
        return x, y


class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()

        self.n_channel = 1
        self.dim_h = int(config['VAE-GAN']['dim_h'])
        self.n_z = int(config['VAE-GAN']['latent_size'])
        
        
        # input:([100, 8])
        self.proj = nn.Sequential(  # ([100, 131072])
            nn.Linear(self.n_z, self.dim_h * 16 * 1 * 1),
            nn.ReLU()
        )

        self.main = nn.Sequential(
            nn.ConvTranspose2d(self.dim_h * 16, self.dim_h * 8, 4),  # ([100, 1024, 4, 4])
            nn.BatchNorm2d(self.dim_h * 8),
            nn.ReLU(True),

            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, kernel_size=5, stride=3,output_padding=2),  # ([100, 512, 16, 16])
            nn.BatchNorm2d(self.dim_h * 4),
            nn.ReLU(True),

            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, kernel_size=4, stride=4,output_padding=0),  # ([100, 256, 64, 64])
            nn.BatchNorm2d(self.dim_h * 2),
            nn.ReLU(True),

            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h * 1, kernel_size=4, stride=4,output_padding=0),  # ([100, 128, 256, 256])
            nn.BatchNorm2d(self.dim_h * 1),
            nn.ReLU(True),

            nn.ConvTranspose2d(self.dim_h * 1, 1, kernel_size=4, stride=2, padding=1, output_padding=0),  # ([100, 1, 512, 512])
            nn.BatchNorm2d(1),
            nn.ReLU(True),
        )

    
    def forward(self, x):
        # x = self.proj(x)
        x = x.view(batch, self.dim_h * 16, 1, 1)  # ([100, 2048, 1, 1])

        x = self.main(x)
        return x

class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        # input: ([batch, 2048])
        self.main = nn.Sequential(
            nn.Linear(2048, 512),
            nn.Dropout(0.5),

            nn.Linear(512, 256),
            nn.Dropout(0.5),

            nn.Linear(256, 128),
            nn.Dropout(0.5),
            
            nn.Linear(128, 8),
        )
        
    def forward(self, x):
        x = self.main(x)
        return x