import sys
import os
current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)


from utils import utils

import torch
from tqdm import tqdm
from sklearn.metrics import recall_score, f1_score, accuracy_score

from torch.optim import Adam
from torch.optim.lr_scheduler import StepLR
from torch.autograd import Variable

import configparser
import pandas as pd
import torch.nn as nn

# 创建一个配置解析器对象
config = configparser.ConfigParser()
# 读取INI文件
config.read('./config/config.ini')


one = torch.Tensor([1]).to(utils.device())
mone = (one * -1).to(utils.device())


###############################################################################################
#
#
# 训练AutoEncoder
#
#
###############################################################################################

def train_autoencoder(encoder, decoder, train_loader, folder_path):

    print('VAE-GAN Model Training begin')

    AutoEncoder_Losses = []
    autoencoder_criterion = nn.MSELoss()
    num_epochs = int(config['VAE-GAN']['epoch_autoencoder'])

    for epoch in range(num_epochs):

        running_loss = 0.0
        
        encoder.train()
        decoder.train()
        # discriminator.train()

        enc_optim = Adam(encoder.parameters(), lr = float(config['VAE-GAN']['learning_rate_autoencoder']))
        dec_optim = Adam(decoder.parameters(), lr=float(config['VAE-GAN']['learning_rate_autoencoder']))
        # dis_optim = Adam(discriminator.parameters(), lr= 0.5 * float(config['VAE-GAN']['learning_rate']))

        enc_scheduler = StepLR(enc_optim, step_size=30, gamma=0.5)
        dec_scheduler = StepLR(dec_optim, step_size=30, gamma=0.5)
        # dis_scheduler = StepLR(dis_optim, step_size=30, gamma=0.5)


        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100):

            # 设置device
            inputs = inputs.to(utils.device())

            # 清零梯度
            encoder.zero_grad()
            decoder.zero_grad()
            # discriminator.zero_grad()

            # ======== Train Discriminator ======== #

            # frozen_params(decoder)
            # frozen_params(encoder)
            # free_params(discriminator)

            # z_fake = torch.randn(inputs.size()[0], int(config['VAE-GAN']['latent_size'])) * int(config['VAE-GAN']['sigma'])

            # if torch.cuda.is_available():
            #     z_fake = z_fake.cuda()

            # d_fake = discriminator(z_fake)

            # z_real = encoder(inputs)
            # d_real = discriminator(z_real)

            # torch.log(d_fake).mean().backward(mone)
            # torch.log(1 - d_real).mean().backward(mone)

            # dis_optim.step()


            # ======== Train Generator ======== #

            # free_params(decoder)
            # free_params(encoder)
            # frozen_params(discriminator)

            z_real, embedding_input = encoder(inputs)
            x_recon = decoder(z_real)
            # d_real = discriminator(encoder(Variable(inputs.data)))

            recon_loss = autoencoder_criterion(x_recon, embedding_input)
            # d_loss = float(config['VAE-GAN']['LAMBDA']) * (torch.log(d_real)).mean()

            recon_loss.backward()
            # d_loss.backward(mone)

            enc_optim.step()
            dec_optim.step()

            running_loss += recon_loss.item()

        # 计算本次 epoch 的平均损失
        avg_loss = running_loss / len(train_loader)

        # 保存所有 epoch 的损失
        AutoEncoder_Losses.append(avg_loss)
                
        print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f}')
    
    utils.model_save(folder_path, (encoder,decoder,))

    print('VAE-GAN Model Training finished')

    return AutoEncoder_Losses


def free_params(module: nn.Module):
    for p in module.parameters():
        p.requires_grad = True

def frozen_params(module: nn.Module):
    for p in module.parameters():
        p.requires_grad = False




###############################################################################################
#
#
# 训练分类器
#
#
###############################################################################################

def train_ClaModel(encoder, cla, train_loader, folder_path):

    # 设置loss
    criterion_claModel = nn.CrossEntropyLoss()
    ClaModel_losses = []
    num_epochs = int(config['VAE-GAN']['epoch_cla'])

    print('ClaModel Training begin')
    for epoch in range(num_epochs):

        cla.train()
        cla_optim = Adam(cla.parameters(), lr = float(config['VAE-GAN']['learning_rate_cla']))

        running_loss = 0.0
        true_labels = []
        predicted_labels = []

        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100):

            # 设置device
            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            # 清零梯度
            cla_optim.zero_grad()

            # 正向传播
            encoder_latent, _ = encoder(inputs)
            outputs = cla(encoder_latent)
            loss = criterion_claModel(outputs, labels)

            # 反向传播和优化
            loss.backward()
            cla_optim.step()

            # 计算损失
            running_loss += loss.item()

            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            _, predicted = torch.max(outputs, 1)
            predicted_labels.extend(predicted.tolist())

        # 计算本次 epoch 的平均损失
        avg_loss = running_loss / len(train_loader)

        # 保存所有 epoch 的损失
        ClaModel_losses.append(avg_loss)

        # 计算召回率、F1-Score 和准确度
        recall = recall_score(true_labels, predicted_labels, average='weighted')
        f1 = f1_score(true_labels, predicted_labels, average='weighted')
        accuracy = accuracy_score(true_labels, predicted_labels)

        # 输出本次 epoch 的指标
        print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f} - Recall: {recall:.4f} - F1-Score: {f1:.4f} - Accuracy: {accuracy:.4f}')

    utils.model_save(folder_path, (cla,))

    print('ClaModel Training finished')

    return ClaModel_losses


###############################################################################################
#
#
# 测试
#
#
###############################################################################################

def test(encoder, cla, test_loader):

    # 设置模型为评估模式
    encoder.eval()
    cla.eval()

    true_labels = []
    predicted_labels = []

    with torch.no_grad():  # 不需要计算梯度
        for inputs, labels in test_loader:

            # 设置device
            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            # 获取模型输出结果
            encoder_outputs, _ = encoder(inputs)
            outputs = cla(encoder_outputs)

            _, predicted = torch.max(outputs, 1)
            
            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            predicted_labels.extend(predicted.tolist())

    # 计算召回率、F1-Score 和准确度
    recall = recall_score(true_labels, predicted_labels, average='weighted')
    f1 = f1_score(true_labels, predicted_labels, average='weighted')
    accuracy = accuracy_score(true_labels, predicted_labels)

    # 输出测试指标
    print(f'Test Accuracy: {accuracy:.4f}')
    print(f'Test Recall: {recall:.4f}')
    print(f'Test F1-Score: {f1:.4f}')

    return recall, f1, accuracy


###############################################################################################
#
#
# Encoder消融
#
#
###############################################################################################

def EncoderClaTrain(encoder, cla, train_loader, folder_path):

    # 设置loss
    criterion_claModel = nn.CrossEntropyLoss()
    ClaModel_losses = []
    num_epochs = int(config['VAE-GAN']['epoch_cla'])

    print('EncoderClaModel Training begin')
    for epoch in range(num_epochs):
        
        encoder.train()
        cla.train()
        encoder_optim = Adam(encoder.parameters(), lr = float(config['VAE-GAN']['learning_rate_autoencoder']))
        cla_optim = Adam(cla.parameters(), lr = float(config['VAE-GAN']['learning_rate_cla']))

        running_loss = 0.0
        true_labels = []
        predicted_labels = []

        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100):

            # 设置device
            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            # 清零梯度
            encoder_optim.zero_grad()
            cla_optim.zero_grad()

            # 正向传播
            encoder_latent, _ = encoder(inputs)
            outputs = cla(encoder_latent)
            loss = criterion_claModel(outputs, labels)

            # 反向传播和优化
            loss.backward()
            encoder_optim.step()
            cla_optim.step()

            # 计算损失
            running_loss += loss.item()

            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            _, predicted = torch.max(outputs, 1)
            predicted_labels.extend(predicted.tolist())

        # 计算本次 epoch 的平均损失
        avg_loss = running_loss / len(train_loader)

        # 保存所有 epoch 的损失
        ClaModel_losses.append(avg_loss)

        # 计算召回率、F1-Score 和准确度
        recall = recall_score(true_labels, predicted_labels, average='weighted')
        f1 = f1_score(true_labels, predicted_labels, average='weighted')
        accuracy = accuracy_score(true_labels, predicted_labels)

        # 输出本次 epoch 的指标
        print(f'Epoch [{epoch + 1}/{num_epochs}] - Loss: {avg_loss:.4f} - Recall: {recall:.4f} - F1-Score: {f1:.4f} - Accuracy: {accuracy:.4f}')

    utils.model_save(folder_path, (encoder, cla,))

    print('ClaModel Training finished')

    return ClaModel_losses


###############################################################################################
#
#
# AutoEncoder和Cla一起训练
#
#
###############################################################################################

def AutoEncoderClaTrain(encoder, decoder, cla, train_loader, folder_path):

    print('AutoEncoderClaTrain Training begin')

    autoencoder_criterion = nn.MSELoss()
    cla_criterion = nn.CrossEntropyLoss()

    AutoEncoder_Losses = []
    Cla_Losses = []

    num_epochs = int(config['VAE-GAN']['epoch_autoencoder'])

    for epoch in range(num_epochs):

        ae_running_loss = 0.0
        cla_running_loss = 0.0

        true_labels = []
        predicted_labels = []
        
        encoder.train()
        decoder.train()
        cla.train()

        enc_optim = Adam(encoder.parameters(), lr = float(config['VAE-GAN']['learning_rate_autoencoder']))
        dec_optim = Adam(decoder.parameters(), lr=float(config['VAE-GAN']['learning_rate_autoencoder']))
        cla_optim = Adam(cla.parameters(), lr=float(config['VAE-GAN']['learning_rate_cla']))

        enc_scheduler = StepLR(enc_optim, step_size=30, gamma=0.5)
        dec_scheduler = StepLR(dec_optim, step_size=30, gamma=0.5)
        cla_scheduler = StepLR(cla_optim, step_size=30, gamma=0.5)


        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100):

            # 设置device
            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            # 清零梯度
            encoder.zero_grad()
            decoder.zero_grad()

            z_real, embedding_input = encoder(inputs)
            x_recon = decoder(z_real)

            recon_loss = autoencoder_criterion(x_recon, embedding_input)

            recon_loss.backward()
            enc_optim.step()
            dec_optim.step()


            # 清零梯度
            encoder.zero_grad()
            cla.zero_grad()

            # 正向传播
            encoder_latent, _ = encoder(inputs)
            outputs = cla(encoder_latent)
            cla_loss = cla_criterion(outputs, labels)

            # 反向传播和优化
            cla_loss.backward()
            enc_optim.step()
            cla_optim.step()


            ae_running_loss += recon_loss.item()
            cla_running_loss += cla_loss.item()

            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            _, predicted = torch.max(outputs, 1)
            predicted_labels.extend(predicted.tolist())


        # 计算召回率、F1-Score 和准确度
        recall = recall_score(true_labels, predicted_labels, average='weighted')
        f1 = f1_score(true_labels, predicted_labels, average='weighted')
        accuracy = accuracy_score(true_labels, predicted_labels)

        # 计算本次 epoch 的平均损失
        ae_avg_loss = ae_running_loss / len(train_loader)
        cla_avg_loss = cla_running_loss / len(train_loader)

        # 保存所有 epoch 的损失
        AutoEncoder_Losses.append(ae_avg_loss)
        Cla_Losses.append(cla_avg_loss)
                
        print(f'Epoch [{epoch + 1}/{num_epochs}] - AutoEncoder_Losses: {ae_avg_loss:.4f} - Cla_Losses: {cla_avg_loss:.4f} - Recall: {recall:.4f} - F1-Score: {f1:.4f} - Accuracy: {accuracy:.4f}')
    
    utils.model_save(folder_path, (encoder,decoder,cla))

    print('AutoEncoderClaTrain Training finished')

    return AutoEncoder_Losses, Cla_Losses
