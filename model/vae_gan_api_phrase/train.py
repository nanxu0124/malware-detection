import sys
import os
current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)


from utils import utils
import torch
import configparser
import pandas as pd
import torch.nn as nn
from tqdm import tqdm
from utils import utils
from torch.optim import Adam
import torch.nn.functional as F
from torch.optim.lr_scheduler import StepLR
from torch.autograd import Variable
from sklearn.metrics import recall_score, f1_score, accuracy_score


config = configparser.ConfigParser()
config.read(os.path.join(project_dir,os.path.join("config", "config.ini")))

batch = int(config['vae_gan_api_phrase']['batch_size'])

def free_params(module: nn.Module):
    for p in module.parameters():
        p.requires_grad = True

def frozen_params(module: nn.Module):
    for p in module.parameters():
        p.requires_grad = False


def train(encoder, decoder,api_phrase, cla, train_dataloader, test_dataloader, folder_path):

    print('Train begin')

    enc_optim = Adam(encoder.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))
    dec_optim = Adam(decoder.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))
    cla_optim = Adam(cla.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))
    api_phrase_optim = Adam(api_phrase.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))


    vae_losses = []
    cla_losses = []
    api_phrase_losses = []
    vae_criterion = nn.MSELoss()
    cla_criterion = nn.CrossEntropyLoss()
    api_phrase_criterion = nn.CrossEntropyLoss()

    epochs = int(config['vae_gan_api_phrase']['epoch'])

    for epoch in range(epochs):

        if epoch % 1 == 0 and epoch != 0:
            recall, f1, accuracy = test(encoder, cla, api_phrase, test_dataloader)
            # if accuracy > 0.85:
            #     utils.model_save_v2(folder_path, (encoder, decoder, cla, ), accuracy)

        vae_running_loss = 0.0
        total_similarity = 0.0
        cla_running_loss = 0.0
        api_phrase_running_losses = 0.0

        true_labels = []
        predicted_labels = []

        encoder.train()
        decoder.train()
        cla.train()
        api_phrase.train()

        for inputs, labels in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}', ncols=100):

            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            ## 训练vae
            # enc_optim.zero_grad()
            # dec_optim.zero_grad()

            # free_params(decoder)
            # free_params(encoder)
            # frozen_params(cla)
            # frozen_params(api_phrase)

            # embed_x, z, x_mu, x_log, y_mu = encoder(inputs)
            # recon_x = decoder(z)
            # recon_loss = vae_criterion(recon_x, embed_x)
            # similarity = utils.SimiliarError(recon_x, embed_x)
            # kl_loss = 0.0 # -0.5 * torch.sum(1 + log - mu.pow(2) - log.exp())

            # vae_loss = recon_loss + kl_loss
            # vae_loss.backward()
            # enc_optim.step()
            # dec_optim.step()


            ## 训练 api_phrase cla
            enc_optim.zero_grad()
            cla_optim.zero_grad()

            free_params(cla)
            free_params(encoder)
            free_params(api_phrase)
            frozen_params(decoder)

            embed_x, z, x_mu, x_log, y_mu = encoder(inputs)
            api_phrase_output = api_phrase(inputs)
            
            cla_input = torch.cat([y_mu, api_phrase_output], dim=1)
            outputs = cla(cla_input)
            cla_loss = cla_criterion(outputs, labels)

            cla_loss.backward()
            enc_optim.step()
            cla_optim.step()

            # vae_running_loss += vae_loss.item()
            cla_running_loss += cla_loss.item()
            # total_similarity += similarity.item()

            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            _, predicted = torch.max(outputs, 1)
            predicted_labels.extend(predicted.tolist())


        # 计算召回率、F1-Score 和准确度
        recall = recall_score(true_labels, predicted_labels, average='weighted')
        f1 = f1_score(true_labels, predicted_labels, average='weighted')
        accuracy = accuracy_score(true_labels, predicted_labels)

        vae_avg_loss = vae_running_loss / len(train_dataloader)
        avg_similarity = total_similarity / len(train_dataloader)
        cla_avg_loss = cla_running_loss / len(train_dataloader)
        vae_losses.append(vae_avg_loss)
        cla_losses.append(cla_avg_loss)

        print(f'Epoch [{epoch + 1}/{epochs}] - vae_losses: {vae_avg_loss:.4f} - cla_losses: {cla_avg_loss:.4f} - avg_similarity: {avg_similarity:.4f} - Recall: {recall:.4f} - F1-Score: {f1:.4f} - Accuracy: {accuracy:.4f}')
    
    utils.model_save(folder_path, (encoder, decoder, cla, ))
    print('Train finished')
    return vae_losses, cla_losses


def test(encoder, cla, api_phrase, test_dataloader):
    
    encoder.eval()
    cla.eval()

    true_labels = []
    predicted_labels = []

    with torch.no_grad():  # 不需要计算梯度
        for inputs, labels in test_dataloader:

            # 设置device
            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            # 获取模型输出结果
            embed_x, z, x_mu, x_log, y_mu = encoder(inputs)
            api_phrase_output = api_phrase(inputs)
            
            cla_input = torch.cat(y_mu, api_phrase_output)
            outputs = cla(cla_input)

            _, predicted = torch.max(outputs, 1)
            
            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            predicted_labels.extend(predicted.tolist())

    # 计算召回率、F1-Score 和准确度
    recall = recall_score(true_labels, predicted_labels, average='weighted')
    f1 = f1_score(true_labels, predicted_labels, average='weighted')
    accuracy = accuracy_score(true_labels, predicted_labels)

    # 输出测试指标
    print(f'Test Accuracy: {accuracy:.4f}')
    print(f'Test Recall: {recall:.4f}')
    print(f'Test F1-Score: {f1:.4f}')

    return recall, f1, accuracy

