import sys
import os
current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)


from utils import utils
import torch
import configparser
import pandas as pd
import torch.nn as nn
from tqdm import tqdm
from utils import utils
from torch.optim import Adam
import torch.nn.functional as F
from torch.optim.lr_scheduler import StepLR
from torch.autograd import Variable
from sklearn.metrics import recall_score, f1_score, accuracy_score


config = configparser.ConfigParser()
config.read(os.path.join(project_dir,os.path.join("config", "config.ini")))

batch = int(config['vae_gan_api_phrase']['batch_size'])
sigma = 1

def free_params(module: nn.Module):
    for p in module.parameters():
        p.requires_grad = True

def frozen_params(module: nn.Module):
    for p in module.parameters():
        p.requires_grad = False


def train(encoder, decoder, cla, dis, api_phrase, train_dataloader, test_dataloader, folder_path):

    print('Train begin')

    enc_optim = Adam(encoder.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))
    dec_optim = Adam(decoder.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))
    cla_optim = Adam(cla.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))
    dis_optim = Adam(dis.parameters(), lr =float(config['vae_gan_api_phrase']['learning_rate']))

    enc_scheduler = StepLR(enc_optim, step_size=10, gamma=0.8)
    dec_scheduler = StepLR(dec_optim, step_size=10, gamma=0.8)
    cla_scheduler = StepLR(cla_optim, step_size=10, gamma=0.8)
    dis_scheduler = StepLR(dis_optim, step_size=10, gamma=0.8)

    vae_losses = []
    cla_losses = []
    dis_losses = []
    vae_criterion = nn.MSELoss()
    cla_criterion = nn.CrossEntropyLoss()

    epochs = int(config['vae_gan_api_phrase']['epoch'])

    for epoch in range(epochs):

        if epoch % 2 == 0 and epoch != 0:
            recall, f1, accuracy = test(encoder, cla, api_phrase, test_dataloader)
            if accuracy > 0.85:
                utils.model_save_v2(folder_path, (encoder, decoder, cla, dis, ), accuracy)

        vae_running_loss = 0.0
        total_similarity = 0.0
        cla_running_loss = 0.0
        dis_running_loss = 0.0
        true_labels = []
        predicted_labels = []

        encoder.train()
        decoder.train()
        cla.train()
        dis.train()

        for inputs, labels in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}', ncols=100):

            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            ## 训练vae
            enc_optim.zero_grad()
            dec_optim.zero_grad()

            free_params(decoder)
            free_params(encoder)
            frozen_params(cla)
            frozen_params(dis)

            mu, log, z, embed_x = encoder(inputs)
            recon_x = decoder(z)
            recon_loss = vae_criterion(recon_x, embed_x)
            similarity = utils.SimiliarError(recon_x, embed_x)
            kl_loss = 0.0 # -0.5 * torch.sum(1 + log - mu.pow(2) - log.exp())

            vae_loss = recon_loss + kl_loss
            vae_loss.backward()
            enc_optim.step()
            dec_optim.step()

            ## 训练dis
            dis_optim.zero_grad()

            free_params(dis)
            frozen_params(decoder)
            frozen_params(encoder)
            frozen_params(cla)

            fake_info = torch.randint_like(inputs, low=0, high=299, dtype=torch.long).to(utils.device())
            _, _, z_fake, embed_x = encoder(fake_info)
            d_fake = dis(z_fake)

            _, _, z, embed_x = encoder(inputs)
            d_real = dis(z)

            fake_loss = F.binary_cross_entropy_with_logits(d_fake, torch.zeros_like(d_fake))
            real_loss = F.binary_cross_entropy_with_logits(d_real, torch.ones_like(d_real))
            fake_loss = torch.mean(fake_loss)
            real_loss = torch.mean(real_loss)

            d_loss = fake_loss + real_loss
            d_loss.backward()
            dis_optim.step()

            ## 训练cla
            enc_optim.zero_grad()
            cla_optim.zero_grad()

            free_params(cla)
            free_params(encoder)
            frozen_params(decoder)
            frozen_params(dis)

            mu, log, z, embed_x = encoder(inputs)
            x = api_phrase(embed_x)
            cla_input = torch.cat((z, x), dim=1)
            outputs = cla(cla_input)
            cla_loss = cla_criterion(outputs, labels)

            cla_loss.backward()
            enc_optim.step()
            cla_optim.step()

            vae_running_loss += vae_loss.item()
            cla_running_loss += cla_loss.item()
            total_similarity += similarity.item()
            dis_running_loss += d_loss.item()

            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            _, predicted = torch.max(outputs, 1)
            predicted_labels.extend(predicted.tolist())
        
        enc_scheduler.step()
        dec_scheduler.step()
        cla_scheduler.step()
        dis_scheduler.step()


        # 计算召回率、F1-Score 和准确度
        recall = recall_score(true_labels, predicted_labels, average='weighted')
        f1 = f1_score(true_labels, predicted_labels, average='weighted')
        accuracy = accuracy_score(true_labels, predicted_labels)

        vae_avg_loss = vae_running_loss / len(train_dataloader)
        avg_similarity = total_similarity / len(train_dataloader)
        cla_avg_loss = cla_running_loss / len(train_dataloader)
        dis_avg_loss = dis_running_loss / len(train_dataloader)
        vae_losses.append(vae_avg_loss)
        cla_losses.append(cla_avg_loss)
        dis_losses.append(dis_avg_loss)

        print(f'Epoch [{epoch + 1}/{epochs}] - vae_losses: {vae_avg_loss:.4f} - cla_losses: {cla_avg_loss:.4f} - dis_losses: {dis_avg_loss:.4f} - avg_similarity: {avg_similarity:.4f} - Recall: {recall:.4f} - F1-Score: {f1:.4f} - Accuracy: {accuracy:.4f}')
    
    utils.model_save(folder_path, (encoder, decoder, cla, dis, ))
    print('Train finished')
    return vae_losses, cla_losses, dis_losses


def test(encoder, cla, api_phrase, test_dataloader):
    
    encoder.eval()
    cla.eval()

    true_labels = []
    predicted_labels = []

    with torch.no_grad():  # 不需要计算梯度
        for inputs, labels in test_dataloader:

            # 设置device
            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            # 获取模型输出结果
            mu, log, z, embed_x = encoder(inputs)
            x = api_phrase(embed_x)
            cla_input = torch.cat((z, x), dim=1)
            outputs = cla(cla_input)

            _, predicted = torch.max(outputs, 1)
            
            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            predicted_labels.extend(predicted.tolist())

    # 计算召回率、F1-Score 和准确度
    recall = recall_score(true_labels, predicted_labels, average='weighted')
    f1 = f1_score(true_labels, predicted_labels, average='weighted')
    accuracy = accuracy_score(true_labels, predicted_labels)

    # 输出测试指标
    print(f'Test Accuracy: {accuracy:.4f}')
    print(f'Test Recall: {recall:.4f}')
    print(f'Test F1-Score: {f1:.4f}')

    return recall, f1, accuracy




def train_vae(encoder, decoder, cla, train_dataloader, folder_path):

    print('Train begin')

    enc_optim = Adam(encoder.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))
    dec_optim = Adam(decoder.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))
    cla_optim = Adam(cla.parameters(), lr=float(config['vae_gan_api_phrase']['learning_rate']))

    vae_losses = []
    cla_losses = []
    vae_criterion = nn.MSELoss()
    cla_criterion = nn.CrossEntropyLoss()

    epochs = int(config['vae_gan_api_phrase']['epoch'])

    for epoch in range(epochs):

        vae_running_loss = 0.0
        total_similarity = 0.0
        cla_running_loss = 0.0
        true_labels = []
        predicted_labels = []

        encoder.train()
        decoder.train()
        cla.train()

        for inputs, labels in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}', ncols=100):

            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            ## 训练vae
            enc_optim.zero_grad()
            dec_optim.zero_grad()

            free_params(decoder)
            free_params(encoder)
            frozen_params(cla)

            mu, log, z, embed_x = encoder(inputs)
            recon_x = decoder(z)
            recon_loss = vae_criterion(recon_x, embed_x)
            similarity = utils.SimiliarError(recon_x, embed_x)
            kl_loss = 0.0 # -0.5 * torch.sum(1 + log - mu.pow(2) - log.exp())

            vae_loss = recon_loss + kl_loss
            vae_loss.backward()
            enc_optim.step()
            dec_optim.step()

            ## 训练cla
            enc_optim.zero_grad()
            cla_optim.zero_grad()

            free_params(cla)
            free_params(encoder)
            frozen_params(decoder)

            mu, log, z, embed_x = encoder(inputs)
            outputs = cla(z)
            cla_loss = cla_criterion(outputs, labels)

            cla_loss.backward()
            enc_optim.step()
            cla_optim.step()

            vae_running_loss += vae_loss.item()
            cla_running_loss += cla_loss.item()
            total_similarity += similarity.item()

            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            _, predicted = torch.max(outputs, 1)
            predicted_labels.extend(predicted.tolist())


        # 计算召回率、F1-Score 和准确度
        recall = recall_score(true_labels, predicted_labels, average='weighted')
        f1 = f1_score(true_labels, predicted_labels, average='weighted')
        accuracy = accuracy_score(true_labels, predicted_labels)

        vae_avg_loss = vae_running_loss / len(train_dataloader)
        avg_similarity = total_similarity / len(train_dataloader)
        cla_avg_loss = cla_running_loss / len(train_dataloader)
        vae_losses.append(vae_avg_loss)
        cla_losses.append(cla_avg_loss)

        print(f'Epoch [{epoch + 1}/{epochs}] - vae_losses: {vae_avg_loss:.4f} - cla_losses: {cla_avg_loss:.4f} - avg_similarity: {avg_similarity:.4f} - Recall: {recall:.4f} - F1-Score: {f1:.4f} - Accuracy: {accuracy:.4f}')
    
    utils.model_save(folder_path, (encoder, decoder, cla, ))
    print('Train finished')
    return vae_losses, cla_losses
