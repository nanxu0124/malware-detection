import sys
import os

current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)

import torch
import torch.nn as nn
import configparser
import torch.nn.functional as F

from data.ali_data.ali_dataset import vocab_pkl

# 创建一个配置解析器对象
config = configparser.ConfigParser()
# 读取INI文件
config.read(os.path.join(project_dir, os.path.join("config", "config.ini")))


vocab_size = len(vocab_pkl)
dim = int(config['data']['embedding_dim'])
max_len = int(config['data']['sequence_max_len'])



class Statistic(nn.Module):
    def __init__(self):
        super(Statistic, self).__init__()
        self.cla = nn.Sequential(

            nn.Linear(80, 32),
            nn.ReLU(),

            nn.Linear(32, 6)
        )
        
    def forward(self, x):
        x = self.cla(x)
        return x
    

class MultiCNN(nn.Module):
    def __init__(self):
        super(MultiCNN, self).__init__()

        self.embedder1 = nn.Embedding(vocab_size, dim, padding_idx=vocab_pkl.PAD)

        self.cnn1_1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, dim), stride=1, padding=(1, 0))
        self.cnn1_2 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(4, dim), stride=1)
        self.cnn1_3 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(5, dim), stride=1, padding=(2, 0))

        self.lstm = nn.LSTM(input_size=96, hidden_size=64, batch_first=True, bidirectional=True)

        self.lin1 = nn.Linear(128, 64)
        self.lin2 = nn.Linear(64, 6)


    def forward(self, x):

        x = self.embedder1(x)
        x= x.unsqueeze(1)

        pad = nn.ZeroPad2d(padding=(0, 0, 2, 1))
        x_pad = pad(x)

        x_name_cnn1 = F.relu(self.cnn1_1(x)).squeeze(-1).permute(0, 2, 1)
        x_name_cnn2 = F.relu(self.cnn1_2(x_pad)).squeeze(-1).permute(0, 2, 1)
        x_name_cnn3 = F.relu(self.cnn1_3(x)).squeeze(-1).permute(0, 2, 1)

        x = torch.cat([x_name_cnn1, x_name_cnn2, x_name_cnn3], dim=-1)

        x, (h_n, c_n) = self.lstm(x)

        output_fw = h_n[-2, :, :]
        output_bw = h_n[-1, :, :]

        x = torch.cat([output_fw, output_bw], dim=-1)
        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=0.2, training=self.training)
        x = self.lin2(x)


        return x


class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()

        self.embedding = nn.Embedding(vocab_size, dim, padding_idx=vocab_pkl.PAD)

        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            # nn.MaxPool2d(2),

            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),

            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )

        self.lstm = nn.LSTM(input_size=64, hidden_size=60, batch_first=True, bidirectional=True)
    
    def mu(self, x):
        x, (h_n, c_n) = self.lstm(x)
        output_fw = h_n[-2, :, :]
        output_bw = h_n[-1, :, :]

        y = torch.cat([output_fw, output_bw], dim=-1)

        return y
      
    def log(self, x):
        x, (h_n, c_n) = self.lstm(x)
        output_fw = h_n[-2, :, :]
        output_bw = h_n[-1, :, :]

        y = torch.cat([output_fw, output_bw], dim=-1)

        return y 

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):

        x = self.embedding(x)
        x = x.view(x.size(0), 1, max_len, dim)
        embed_x = x
    
        x = self.conv(embed_x)
        x = x.squeeze().permute(0, 2, 1)
        
        x = self.mu(x)

        return x

class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()

        self.cla = nn.Sequential(
            # nn.Linear(420, 256),
            # nn.ReLU(),
            # # nn.Dropout(),

            # nn.Linear(256, 128),
            # nn.ReLU(),

            nn.Linear(200, 6),
        )
        
    def forward(self, x):
        return self.cla(x)