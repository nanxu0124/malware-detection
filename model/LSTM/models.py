import sys
import os

current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)

import torch
import torch.nn as nn
import torch.nn.functional as F
import configparser

from data.ali_data import ali_dataset

# 创建一个配置解析器对象
config = configparser.ConfigParser()
# 读取INI文件
config.read('./config/config.ini')


vocab_size = len(ali_dataset.vocab_pkl)
dim = int(config['data']['embedding_dim'])
n_class = 8
max_len = int(config['data']['sequence_max_len'])


class LSTM(nn.Module):
    def __init__(self):
        super(LSTM, self).__init__()

        # 将实现训练好的词向量导入
        self.embeding = nn.Embedding(vocab_size, dim, padding_idx=ali_dataset.vocab_pkl.PAD)

        self.lstm = nn.LSTM(input_size=dim, hidden_size=128, num_layers=2, batch_first=True, bidirectional=True,
                            dropout=0.5)
        self.fc1 = nn.Linear(128 * 2, 128)
        self.fc2 = nn.Linear(128, 8)

    def forward(self, input):
        """
        :param input:[batch_size,max_len]
        :return:
        """
        input_embeded = self.embeding(input)  # input embeded :[batch_size,max_len,dim]

        output, (h_n, c_n) = self.lstm(input_embeded)  # h_n :[4,batch_size,hidden_size]
        # out :[batch_size,hidden_size*2]
        out = torch.cat([h_n[-1, :, :], h_n[-2, :, :]], dim=-1)  # 拼接正向最后一个输出和反向最后一个输出

        # 进行全连接
        out_fc1 = self.fc1(out)
        # # 进行relu
        out_fc1_relu = F.relu(out_fc1)

        # # 全连接
        out_fc2 = self.fc2(out_fc1_relu)  # out :[batch_size,2]
        return F.log_softmax(out_fc2, dim=-1)