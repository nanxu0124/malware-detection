import sys
import os
current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)


from utils import utils

import torch
from tqdm import tqdm
from sklearn.metrics import recall_score, f1_score, accuracy_score

from torch.optim import Adam
from torch.optim.lr_scheduler import StepLR
from torch.autograd import Variable

import configparser
import pandas as pd
import torch.nn as nn

# 创建一个配置解析器对象
config = configparser.ConfigParser()
# 读取INI文件
config.read('./config/config.ini')


###############################################################################################
#
#
# VAE和Cla一起训练
#
#
###############################################################################################
def TrainALL(encoder, decoder, cla, train_loader, folder_path):

    print('TrainALL Training begin')

    vae_criterion = nn.MSELoss()
    cla_criterion = nn.CrossEntropyLoss()
    VAE_Losses = []
    CLA_Losses = []

    num_epochs = int(config['VAEGAN']['epochall'])

    for epoch in range(num_epochs):

        vae_running_loss = 0.0
        cla_running_loss = 0.0
        total_similarity = 0.0

        true_labels = []
        predicted_labels = []
        
        encoder.train()
        decoder.train()
        cla.train()

        enc_optim = Adam(encoder.parameters(), lr=float(config['VAEGAN']['learning_rate']))
        dec_optim = Adam(decoder.parameters(), lr=float(config['VAEGAN']['learning_rate']))
        cla_optim = Adam(cla.parameters(), lr=float(config['VAEGAN']['learning_rate']))


        for inputs, labels in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100):

            # 设置device
            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            # 清零梯度
            enc_optim.zero_grad()
            dec_optim.zero_grad()

            mu, log, z, inputEmbedding = encoder(inputs)
            x_recon = decoder(z)

            # 计算重构损失
            recon_loss = vae_criterion(x_recon, inputEmbedding)
            
            # 计算相似度
            similarity = utils.SimiliarError(inputEmbedding, x_recon)

            # 计算 KL 散度损失
            kl_loss = -0.5 * torch.sum(1 + log - mu.pow(2) - log.exp())

            vae_loss = recon_loss + kl_loss

            vae_loss.backward()
            enc_optim.step()
            dec_optim.step()


            # 清零梯度
            enc_optim.zero_grad()
            cla_optim.zero_grad()

            # 正向传播
            _, _, z, _ = encoder(inputs)
            outputs = cla(z)
            cla_loss = cla_criterion(outputs, labels)

            # 反向传播和优化
            cla_loss.backward()
            enc_optim.step()
            cla_optim.step()


            vae_running_loss += vae_loss.item()
            cla_running_loss += cla_loss.item()
            total_similarity += similarity.item()

            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            _, predicted = torch.max(outputs, 1)
            predicted_labels.extend(predicted.tolist())


        # 计算召回率、F1-Score 和准确度
        recall = recall_score(true_labels, predicted_labels, average='weighted')
        f1 = f1_score(true_labels, predicted_labels, average='weighted')
        accuracy = accuracy_score(true_labels, predicted_labels)

        # 计算本次 epoch 的平均损失
        vae_avg_loss = vae_running_loss / len(train_loader)
        cla_avg_loss = cla_running_loss / len(train_loader)
        avg_similarity = total_similarity / len(train_loader)

        # 保存所有 epoch 的损失
        VAE_Losses.append(vae_avg_loss)
        CLA_Losses.append(cla_avg_loss)
                
        print(f'Epoch [{epoch + 1}/{num_epochs}] - VAE_Losses: {vae_avg_loss:.4f} - CLA_Losses: {cla_avg_loss:.4f} - avg_similarity: {avg_similarity:.4f} - Recall: {recall:.4f} - F1-Score: {f1:.4f} - Accuracy: {accuracy:.4f}')
    
    utils.model_save(folder_path, (encoder, decoder, cla))

    print('AutoEncoderClaTrain Training finished')

    return VAE_Losses, CLA_Losses

###############################################################################################
#
#
# Test
#
#
###############################################################################################

def Test(encoder, cla, test_loader):

    # 设置模型为评估模式
    encoder.eval()
    cla.eval()

    true_labels = []
    predicted_labels = []

    with torch.no_grad():  # 不需要计算梯度
        for inputs, labels in test_loader:

            # 设置device
            inputs = inputs.to(utils.device())
            labels = labels.to(utils.device())

            # 获取模型输出结果
            mu, _, z, _ = encoder(inputs)
            outputs = cla(mu)

            _, predicted = torch.max(outputs, 1)
            
            # 保存真实标签和预测标签以计算指标
            true_labels.extend(labels.tolist())
            predicted_labels.extend(predicted.tolist())

    # 计算召回率、F1-Score 和准确度
    recall = recall_score(true_labels, predicted_labels, average='weighted')
    f1 = f1_score(true_labels, predicted_labels, average='weighted')
    accuracy = accuracy_score(true_labels, predicted_labels)

    # 输出测试指标
    print(f'Test Accuracy: {accuracy:.4f}')
    print(f'Test Recall: {recall:.4f}')
    print(f'Test F1-Score: {f1:.4f}')

    return recall, f1, accuracy
