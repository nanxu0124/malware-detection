import sys
import os

current_dir = os.path.dirname(__file__)
project_dir = os.path.abspath(os.path.join(current_dir, '../..'))
sys.path.append(project_dir)

import torch
import torch.nn as nn
import torch.nn.functional as F
import configparser

from data.ali_data import ali_dataset

# 创建一个配置解析器对象
config = configparser.ConfigParser()
# 读取INI文件
config.read('./config/config.ini')


# 词嵌入参数
vocab_size = len(ali_dataset.vocab_pkl) # 词表长度
embeddingDim = int(config['data']['embedding_dim']) # 词嵌入维度，一般是512
max_len = int(config['data']['sequence_max_len'])   # api截断长度,一般是512


# 训练参数
batch = int(config['VAEGAN']['batchSize']) # 训练批次
channel = int(config['VAEGAN']['channel'])  # channel倍数s


class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()

        self.n_channel = 1
        self.dim_h = 128

        # 词嵌入
        # 将实现训练好的词向量导入
        # input:([batch, 512])
        self.embeding = nn.Embedding(vocab_size, embeddingDim, padding_idx=ali_dataset.vocab_pkl.PAD)    # ([batch, 512, 512])

        # Encoder
        # input:([batch, 1, 512, 512])
        self.encode = nn.Sequential(
            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1),     # ([100, 128, 256, 256])
            nn.ReLU(),
            nn.MaxPool2d(2),    # ([100, 128, 128, 128])

            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1,),     # ([100, 256, 64, 64])
            nn.BatchNorm2d(self.dim_h * 2),
            nn.ReLU(),
            nn.MaxPool2d(2),    # ([100, 256, 32, 32])

            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1), # ([100, 512, 16, 16])
            nn.BatchNorm2d(self.dim_h * 4),
            nn.ReLU(),
            nn.MaxPool2d(2),    # ([100, 512, 8, 8])

            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1), # ([100, 1024, 4, 4])
            nn.BatchNorm2d(self.dim_h * 8),
            nn.ReLU(),
            nn.MaxPool2d(2),    # ([100, 1024, 2, 2])

        )

        self.mu = nn.Sequential(
            nn.Conv2d(self.dim_h * 8, self.dim_h * 16,2 , 1, 0),# ([100, 2048, 1, 1])
            nn.BatchNorm2d(self.dim_h * 16),
            nn.ReLU(),
        )

        self.log = nn.Sequential(
            nn.Conv2d(self.dim_h * 8, self.dim_h * 16,2 , 1, 0),# ([100, 2048, 1, 1])
            nn.BatchNorm2d(self.dim_h * 16),
            nn.ReLU(),
        )

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):

        x = self.embeding(x)
        x = x.view(x.size(0), 1, max_len, embeddingDim)
        inputEmbedding = x

        x = self.encode(x)
        mu = self.mu(x)
        log = self.log(x)

        z = self.reparameterize(mu ,log)

        return mu, log, z, inputEmbedding

class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()

        self.n_channel = 1
        self.dim_h = 128

        self.decode = nn.Sequential(
            nn.ConvTranspose2d(self.dim_h * 16, self.dim_h * 8, 4),  # ([100, 1024, 4, 4])
            nn.BatchNorm2d(self.dim_h * 8),
            nn.ReLU(),

            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, kernel_size=5, stride=3,output_padding=2),  # ([100, 512, 16, 16])
            nn.BatchNorm2d(self.dim_h * 4),
            nn.ReLU(),

            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, kernel_size=4, stride=4,output_padding=0),  # ([100, 256, 64, 64])
            nn.BatchNorm2d(self.dim_h * 2),
            nn.ReLU(),

            nn.ConvTranspose2d(self.dim_h * 2, self.dim_h * 1, kernel_size=4, stride=4,output_padding=0),  # ([100, 128, 256, 256])
            nn.BatchNorm2d(self.dim_h * 1),
            nn.ReLU(),

            nn.ConvTranspose2d(self.dim_h * 1, 1, kernel_size=4, stride=2, padding=1, output_padding=0),  # ([100, 1, 512, 512])
            nn.BatchNorm2d(1),
            nn.ReLU(),
        )

    def forward(self, x):
        x = x.view(batch, self.dim_h * 16, 1, 1)  # ([100, 2048, 1, 1])

        recon_x = self.decode(x)
        return recon_x


class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        # input: ([batch, 2048])
        self.main = nn.Sequential(
            nn.Linear(2048, 512),

            nn.Linear(512, 256),

            nn.Linear(256, 128),
            
            nn.Linear(128, 4),
        )
        
    def forward(self, x):
        x = x.squeeze()
        x = self.main(x)
        return x


class Discriminator(nn.Module):
    pass